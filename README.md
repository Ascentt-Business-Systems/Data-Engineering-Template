# {{cookiecutter.project_name}}

This project follows a well-organized structure to ensure that code, data, and configurations are properly managed, tested, and deployed. Below is an explanation of each folder, its purpose, and how it should be used.


### Folder Descriptions

#### 1. `.github/workflows/`
This folder contains GitHub Actions workflow files that automate testing, deployment, and other processes. By leveraging these CI/CD pipelines, the project can maintain consistent integration and delivery across environments.

- **Common Use**: Define workflows for automating tests and deployment pipelines.
- **Example**: `.github/workflows/main.yml` might define a pipeline for running tests on every pull request.

#### 2. `conf/`
The `conf/` folder stores configuration files for different environments such as development, testing, staging, and production.

- **Subfolders**:
  - `default/`: Holds default settings applicable across all environments.
  - `local/`: Stores configurations specific to the local development environment.

- **Common Use**: Centralize environment settings to control runtime behavior for various stages.

#### 3. `data/`
This folder is reserved for storing datasets throughout the data pipeline. It can be organized into subdirectories such as raw data, processed data, and outputs.

- **Common Use**: Store project datasets, including raw inputs, processed files, and analysis results.

#### 4. `deploy/`
The `deploy/` folder contains deployment-related scripts and configurations, potentially including infrastructure-as-code files such as Terraform scripts or deployment pipelines for cloud infrastructure.

- **Common Use**: Manage and automate project deployment, whether on cloud platforms, Docker, or other environments.

#### 5. `docs/`
The `docs/` folder is used for all project documentation. This may include API documentation, technical specifications, user guides, or architecture diagrams.

- **Common Use**: Document key aspects of the project for developers and end-users.
- **Example**: `docs/architecture.md` might explain the architecture of the data pipeline or application.

#### 6. `notebooks/`
This folder stores Jupyter notebooks used for data exploration, experimentation, or prototyping. Notebooks can provide detailed walkthroughs of analysis or machine learning models.

- **Common Use**: Conduct Exploratory Data Analysis (EDA), quick experiments, and model prototyping.

#### 7. `results/`
The `results/` folder is for storing output files such as analysis results, model predictions, or evaluation reports.

- **Common Use**: Save the results from experiments, analyses, or data processes.

#### 8. `scripts/`
This folder contains reusable scripts for automating tasks like data processing, model training, or environment setup. These scripts are often standalone and can be invoked directly via the command line.

- **Common Use**: Store utility scripts, shell scripts, or Python automation tasks.
- **Example**: `scripts/data_ingest.sh` could be used to download and preprocess datasets from a remote server.

#### 9. `src/`
The `src/` directory is where the core source code of the project resides. Itâ€™s organized into different components based on their functionality.

##### `src/tests/`
The `tests/` folder is for testing your code. Tests are typically divided into categories based on data size or complexity:
- `fixtures/`: Contains predefined datasets or test data that can be used across multiple tests.
- `large/`: Tests for large data inputs or complex use cases.
- `medium/`: Mid-sized tests for intermediate-level data.
- `small/`: Unit tests for functions or components that work with small data or mock inputs.

##### `src/{{cookiecutter.project_name}}_common_utilities/`
This folder contains general-purpose utility functions and configurations that are shared across the project.
- `config/`: Configuration management utilities, such as environment handlers or settings parsers.
- `utils/`: Helper functions that are not directly tied to any specific part of the project but support various functionalities.

##### `src/{{cookiecutter.project_name}}_data/`
This folder focuses on data-related operations, particularly data engineering tasks.

###### `de/` (Data Engineering)
The `de/` subfolder contains the main data engineering code, structured into the following subfolders:

- **`feature/`**: Code related to feature engineering, transforming raw data into features ready for modeling or analysis.
- **`intermediate/`**: Stores intermediate processed data as part of a multi-step data pipeline.
- **`model_input/`**: Contains data that has been processed and is ready for use in machine learning model training.
- **`primary/`**: Holds the initial structured data that forms the basis for further data engineering.
- **`raw/`**: Stores the raw data ingested from various sources before any processing has been applied.
- **`schema/`**: Contains data schemas that define the expected structure of datasets, including column names, data types, and validation rules.
- **`utils/`**: Utility functions specifically designed for data engineering tasks, such as data validation or transformation helpers.

---

This structure supports a clean, maintainable approach to data engineering and development by clearly separating different layers of code and data. Each folder has a distinct purpose, contributing to the overall modularity and scalability of the project.
